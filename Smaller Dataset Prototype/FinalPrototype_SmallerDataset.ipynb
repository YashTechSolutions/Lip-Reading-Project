{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cGW5TLlbqjo"
   },
   "source": [
    "INITIAL SETUP I: INSTALLING PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHyGgcKRbqjr",
    "outputId": "ac7a12d4-61f8-4394-af01-8021c78a6984"
   },
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install dlib imutils imageio kaggle seaborn tensorflow scikit-image kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IkWR6Snbqjs"
   },
   "source": [
    "INITIAL SETUP II: IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JpNQRFFbTOc4"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import dlib\n",
    "import imutils\n",
    "import imageio\n",
    "from imutils import face_utils\n",
    "from skimage.transform import resize\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv3D, MaxPooling3D, Flatten, Dense, Dropout, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5P68C5vbqju"
   },
   "source": [
    "INITIAL SETUP III: SETTING UP MODEL FILES AND RETIREVING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGdN2Lrjbqju",
    "outputId": "f5a54d1d-deb6-4d61-c7d9-4cca3f814076"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensure the dataset and model file are accessible\n",
    "!mkdir -p /content/dataset\n",
    "!mkdir -p /content/models\n",
    "\n",
    "!mkdir ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "!kaggle datasets download -d apoorvwatsky/miraclvc1 -p /content/dataset --unzip\n",
    "\n",
    "# Download dataset\n",
    "import kagglehub\n",
    "dataset_path = kagglehub.dataset_download(\"apoorvwatsky/miraclvc1\")\n",
    "dataset_dir = \"/root/.cache/kagglehub/datasets/apoorvwatsky/miraclvc1/versions/3/dataset/dataset\"\n",
    "predictor_path = \"/root/.cache/kagglehub/datasets/apoorvwatsky/miraclvc1/versions/3/shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "if not os.path.exists(dataset_dir) or not os.path.exists(predictor_path):\n",
    "    raise FileNotFoundError(\"Dataset directory or shape predictor file not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cSSZM2c6cgDj",
    "outputId": "db3b1110-cd91-4100-941c-607dee62d560"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiLa-bipbqju"
   },
   "source": [
    "DATA MANIPULATION I: DEFINING DATA MANIPULATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WpsTLaGqbqju"
   },
   "outputs": [],
   "source": [
    "# Functions for face cropping\n",
    "def rect_to_bb(rect):\n",
    "    x, y, w, h = rect.left(), rect.top(), rect.right() - rect.left(), rect.bottom() - rect.top()\n",
    "    return x, y, w, h\n",
    "\n",
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "    coords = np.zeros((68, 2), dtype=dtype)\n",
    "    for i in range(68):\n",
    "        coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "    return coords\n",
    "\n",
    "def crop_and_save_image(img, img_path, write_img_path, img_name, predictor):\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    image = cv2.imread(img_path)\n",
    "    image = imutils.resize(image, width=500)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    rects = detector(gray, 1)\n",
    "    if len(rects) != 1:\n",
    "        print(f\"ERROR: Detected {len(rects)} faces.\")\n",
    "        return\n",
    "    shape = predictor(gray, rects[0])\n",
    "    shape = face_utils.shape_to_np(shape)\n",
    "    x, y, w, h = cv2.boundingRect(np.array([shape[48:68]]))\n",
    "    roi = gray[y:y+h, x:x+w]\n",
    "    roi = imutils.resize(roi, width=250)\n",
    "    os.makedirs(os.path.dirname(write_img_path), exist_ok=True)\n",
    "    cv2.imwrite(write_img_path, roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGx-ri_2bqjv"
   },
   "source": [
    "DATA MANIPULATION II: DEFINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o865CBSDbqjv"
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "people = ['F01', 'F02', 'F04', 'F05', 'F06', 'F07', 'F08', 'F09', 'F10', 'F11', 'M01', 'M02', 'M04', 'M07', 'M08']\n",
    "data_types = ['words']\n",
    "folder_enum = ['01', '02', '05','07','08','10']\n",
    "instances = ['01', '02']\n",
    "words = ['Begin', 'Choose', 'Next',  'Start', 'Stop',  'Web']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cc8gQMv2bqjv"
   },
   "source": [
    "DATA MANIPULATION III: CROPING EACH PERSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zUvzTG7bqjv"
   },
   "outputs": [],
   "source": [
    "\n",
    "def crop_one_person():\n",
    "    for person_ID in people:\n",
    "        for data_type in data_types:\n",
    "            for phrase_ID in folder_enum:\n",
    "                for instance_ID in instances:\n",
    "                    directory = os.path.join(dataset_dir, person_ID, data_type, phrase_ID, instance_ID)\n",
    "                    cropped_dir = f'cropped/{person_ID}/{data_type}/{phrase_ID}/{instance_ID}/'\n",
    "                    os.makedirs(cropped_dir, exist_ok=True)\n",
    "                    filelist = os.listdir(directory)\n",
    "                    for img_name in filelist:\n",
    "                        if img_name.startswith('color'):\n",
    "                            image = imageio.imread(os.path.join(directory, img_name))\n",
    "                            crop_and_save_image(\n",
    "                                image,\n",
    "                                os.path.join(directory, img_name),\n",
    "                                os.path.join(cropped_dir, img_name),\n",
    "                                img_name,\n",
    "                                dlib.shape_predictor(predictor_path)\n",
    "                            )\n",
    "\n",
    "# Crop all images\n",
    "crop_one_person()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyO_i-cJbqjv"
   },
   "source": [
    "DATA MANIPULATION IV: PREPARE DATA FOR TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GhbXPMdObqjv",
    "outputId": "806a366d-641f-40d3-ebef-9819c31fdca7"
   },
   "outputs": [],
   "source": [
    "max_seq_length = 22\n",
    "MAX_WIDTH, MAX_HEIGHT = 100, 100\n",
    "UNSEEN_VALIDATION_SPLIT = ['F07', 'M02']\n",
    "UNSEEN_TEST_SPLIT = ['F04', 'M01']\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = [], [], [], [], [], []\n",
    "\n",
    "directory = \"Smaller Dataset Prototype/cropped\"\n",
    "for person_id in people:\n",
    "    for data_type in data_types:\n",
    "        for word_index, word in enumerate(folder_enum):\n",
    "            for iteration in instances:\n",
    "                path = os.path.join(directory, person_id, data_type, word, iteration)\n",
    "                filelist = sorted(os.listdir(path))\n",
    "                sequence = []\n",
    "                for img_name in filelist:\n",
    "                    if img_name.startswith('color'):\n",
    "                        image = imageio.imread(os.path.join(path, img_name))\n",
    "                        image = resize(image, (MAX_WIDTH, MAX_HEIGHT))\n",
    "                        image = (255 * image).astype(np.uint8)\n",
    "                        sequence.append(image)\n",
    "                pad_array = [np.zeros((MAX_WIDTH, MAX_HEIGHT))] * (max_seq_length - len(sequence))\n",
    "                sequence.extend(pad_array)\n",
    "                if person_id in UNSEEN_TEST_SPLIT:\n",
    "                    X_test.append(sequence)\n",
    "                    y_test.append(word_index)\n",
    "                elif person_id in UNSEEN_VALIDATION_SPLIT:\n",
    "                    X_val.append(sequence)\n",
    "                    y_val.append(word_index)\n",
    "                else:\n",
    "                    X_train.append(sequence)\n",
    "                    y_train.append(word_index)\n",
    "\n",
    "X_train, X_val, X_test = map(np.array, (X_train, X_val, X_test))\n",
    "y_train, y_val, y_test = map(lambda y: to_categorical(y, 10), (y_train, y_val, y_test))\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=4)\n",
    "X_val = np.expand_dims(X_val, axis=4)\n",
    "X_test = np.expand_dims(X_test, axis=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7QtRiqIbqjw"
   },
   "source": [
    "MODEL CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgK9JaKBbqjw",
    "outputId": "ad5355c4-4c80-4962-9e47-7f9f2ac1704a"
   },
   "outputs": [],
   "source": [
    "# Model definition using EfficientNet\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable()\n",
    "class EnhancedLipNetWithEfficientNet(Model):\n",
    "    def __init__(self, num_classes=10, dropout_rate=0.3, num_heads=4, key_dim=64):\n",
    "        super().__init__()\n",
    "        self.efficientnet = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(100, 100, 3))\n",
    "        self.efficientnet.trainable = False\n",
    "        self.lstm = layers.Bidirectional(layers.LSTM(128, return_sequences=True))\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "        self.fc = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size, time_steps, height, width, channels = tf.unstack(tf.shape(inputs))\n",
    "\n",
    "        # Reshape the inputs for EfficientNet\n",
    "        inputs_rgb = tf.reshape(inputs, [-1, height, width, channels])\n",
    "        inputs_rgb = tf.image.grayscale_to_rgb(inputs_rgb)  # Convert grayscale to RGB\n",
    "\n",
    "        # Pass through EfficientNet\n",
    "        x = self.efficientnet(inputs_rgb)  # Output shape: (batch_size * time_steps, feature_map_h, feature_map_w, feature_channels)\n",
    "        x = tf.reduce_mean(x, axis=[1, 2])  # Global average pooling (GAP), shape: (batch_size * time_steps, feature_channels)\n",
    "\n",
    "        # Reshape back to temporal sequence format\n",
    "        feature_dim = x.shape[-1]  # Dynamically infer the feature dimension\n",
    "        x = tf.reshape(x, [batch_size, time_steps, feature_dim])\n",
    "\n",
    "        # Process sequence with LSTM and attention\n",
    "        x = self.lstm(x)\n",
    "        x = self.attention(x, x)\n",
    "        x = tf.reduce_mean(x, axis=1)  # Reduce temporal axis\n",
    "\n",
    "        # Final classification layer\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize and compile the model\n",
    "model = EnhancedLipNetWithEfficientNet(num_classes=10)\n",
    "# Compile the model with AdamW optimizer\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=0.001, weight_decay=1e-4)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRzq8OmWbqjw"
   },
   "source": [
    "TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wtzMiUa0bqjw",
    "outputId": "e68eed1b-a945-4e35-e662-2d801a2822cc"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=40, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ag18peN9bqjw"
   },
   "source": [
    "EVALUATION GRAPH AND CLASSIFICATION REPORT POST TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 920
    },
    "id": "OMneVNDQCe_-",
    "outputId": "d2fe2048-4fc4-45e1-e190-05c9624901d8"
   },
   "outputs": [],
   "source": [
    "# Evaluate accuracy and loss graphs\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
    "y_val_true_classes = np.argmax(y_val, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_val_true_classes, y_val_pred_classes)\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt='d')\n",
    "plt.title(\"Validation Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H8B913yNbqjx",
    "outputId": "afe75082-dfba-4259-a850-9526aa22398c"
   },
   "outputs": [],
   "source": [
    "# Classification report for validation set\n",
    "val_classification_report = classification_report(y_val_true_classes, y_val_pred_classes, target_names=words)\n",
    "print(\"Validation Classification Report:\")\n",
    "print(val_classification_report)\n",
    "\n",
    "# Final test evaluation\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_pred_classes = np.argmax(y_test_pred, axis=1)\n",
    "y_test_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "test_accuracy = np.mean(y_test_true_classes == y_test_pred_classes)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "# Test set classification report\n",
    "test_classification_report = classification_report(y_test_true_classes, y_test_pred_classes, target_names=words)\n",
    "print(\"Test Classification Report:\")\n",
    "print(test_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAwJ74crbqjx"
   },
   "source": [
    "SAVING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xg3uxslHkTX6",
    "outputId": "e7f6c27d-85f4-4b7f-e4b0-0b6987ca5d62"
   },
   "outputs": [],
   "source": [
    "# Save the model in Keras format\n",
    "model_save_path = \"lip_reading_model_effnet_final_58.33.keras\"\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved in Keras format to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjjk2nJubqjx"
   },
   "source": [
    "LOADING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAW-PzWEpol-"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model_save_path = \"/content/lip_reading_model_effnet_final_58.33.keras\"\n",
    "new2 = load_model(model_save_path)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "y_test_pred = new2.predict(X_test)\n",
    "y_test_pred_classes = np.argmax(y_test_pred, axis=1)\n",
    "y_test_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "test_accuracy = np.mean(y_test_true_classes == y_test_pred_classes)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "test_classification_report = classification_report(y_test_true_classes, y_test_pred_classes, target_names=words)\n",
    "print(\"Test Classification Report:\")\n",
    "print(test_classification_report)\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_true_classes, y_test_pred_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=words, yticklabels=words)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCg-cfJjpxrU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
